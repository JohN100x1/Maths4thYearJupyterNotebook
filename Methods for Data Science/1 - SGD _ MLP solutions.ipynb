{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent / Multilayer Perceptron\n",
    "\n",
    "The purpose of this notebook is to practice implementing the stochastic gradient descent (SGD) optimisation algorithm and the multilayer perceptron (MLP) model.\n",
    "\n",
    "We will implement the SGD algorithm and MLP model in numpy, and use TensorFlow to train the MLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Stochastic gradient descent (SGD)\n",
    "\n",
    "We consider a linear regression problem of the form\n",
    "$$\n",
    "y = \\beta_0 + \\boldsymbol x^T\\boldsymbol\\beta + \\epsilon\\,,\\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2)\n",
    "$$\n",
    "where $\\boldsymbol x\\in\\mathbb{R}^D$ are inputs and $y\\in\\mathbb{R}$ are noisy observations. The parameter vector $\\boldsymbol\\beta\\in\\mathbb{R}^D$ parametrizes the function.\n",
    "\n",
    "In this tutorial, we assume that we are able to sample data inputs and outputs $(\\boldsymbol x_n, y_n)$, $n=1,\\ldots, K$, and we are interested in finding parameters $\\beta_0$ and  $\\boldsymbol\\beta$ that map the inputs well to the ouputs.\n",
    "\n",
    "From our lectures, we know that the parameters $\\beta_0$ and $\\boldsymbol\\beta$ can be calculated analytically. However, in this section we are interested in computing a numerical solution using the stochastic gradient descent algorithm (SGD).\n",
    "\n",
    "We will start by setting up a generator of data inputs and outputs, that we define by ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data generator\n",
    "\n",
    "true_beta0 = 3.7\n",
    "true_beta = -1.8\n",
    "sigma = 0.5\n",
    "xlim = [-3, 3]\n",
    "\n",
    "def datagen(batch_size):\n",
    "    while True:\n",
    "        x = np.random.uniform(*xlim, (batch_size, 1))  ## <-- shape (batch_size, 1)\n",
    "        noise = np.random.randn(batch_size, 1) * sigma\n",
    "        y = true_beta0 + x * true_beta + noise\n",
    "        yield x, y\n",
    "    \n",
    "train_data = datagen(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pull a batch of training data and plot it, along with the true underyling function\n",
    "\n",
    "x_sample, y_sample = next(train_data)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_sample, y_sample, label='Data samples')\n",
    "plt.plot(x_sample, true_beta0 + x_sample * true_beta, color='C1', label='True fn')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss that we wish to minimise is the expected mean squared error loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\beta_0, \\boldsymbol\\beta) := \\mathbb{E}_{x, y\\sim p_{data}} \\left[\\frac{1}{2}(y - \\beta_0 - \\boldsymbol x^T\\boldsymbol\\beta)^2\\right]\n",
    "$$\n",
    "\n",
    "We first compute the mean squared error loss on a batch of input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDIT THIS FUNCTION\n",
    "def mse_loss(x, y, beta0, beta):\n",
    "    \n",
    "    # x: K x D array of inputs\n",
    "    # y: K x 1 array of outputs\n",
    "    # beta0: Length 1 1-D array for bias parameter\n",
    "    # beta: D x 1 array of parameters\n",
    "    # returns: MSE computed on this batch of inputs and outputs; K x 1 array\n",
    "    \n",
    "    loss = 0.5 * ((y - beta0 - x @ beta)**2).mean() ## <-- SOLUTION\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialise the parameters and plot the initialised regression function\n",
    "\n",
    "beta0, beta = np.array([1.0]), np.array([[-0.5]])\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_sample, y_sample, label='Data samples')\n",
    "plt.plot(x_sample, true_beta0 + x_sample * true_beta, color='C1', label='True fn')\n",
    "plt.plot(x_sample, beta0 + x_sample * beta, color='C2', label='Initialised fn')\n",
    "plt.title(r'Initial parameters: $\\beta_0$: {:.2f}, $\\beta_1$: {:.2f}. MSE loss: {:.4f}'.format(\n",
    "    np.squeeze(beta0), np.squeeze(beta), mse_loss(x_sample, y_sample, beta0, beta))\n",
    "         )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent samples a batch of $K$ input and output samples, and makes a parameter update by  computing the gradient \n",
    "\n",
    "$$\n",
    "\\nabla_{(\\beta_0, \\boldsymbol\\beta)}\\mathcal{L}(\\beta_0^{(i)}, \\boldsymbol\\beta^{(i)} \\mid \\mathcal{X}^{(i)}, \\mathcal{Y}^{(i)}),\n",
    "$$\n",
    "\n",
    "where $\\beta_0^{(i)}, \\boldsymbol\\beta^{(i)}$ are the values of the parameters at the $i$-th iteration of the algorithm, and $\\mathcal{X}^{(i)}, \\mathcal{Y}^{(i)}$ are the $i$-th batch of inputs and outputs.\n",
    "\n",
    "The following function should compute this gradient for a given batch of data, and current parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDIT THIS FUNCTION\n",
    "def grad(x, y, beta0, beta):\n",
    "    \n",
    "    # x: K x D array of inputs\n",
    "    # y: K x 1 array of outputs\n",
    "    # beta0: Length 1 1-D array for bias parameter\n",
    "    # beta: D x 1 array of parameters\n",
    "    # returns: tuple of gradients for beta0 (length 1 1-D array) and beta (D x 1 array)\n",
    "    \n",
    "    g = (y - beta0 - x @ beta)  ## <-- K x 1 array\n",
    "    return -g.mean(axis=0), -(x.T[..., np.newaxis] * g).mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have what we need to run the SGD algorithm for our problem task.\n",
    "\n",
    "Recall that stochastic gradient descent makes the following parameter update at each iteration:\n",
    "\n",
    "$$\n",
    "(\\beta_0^{(i+1)}, \\boldsymbol\\beta^{(i+1)}) = (\\beta_0^{(i)}, \\boldsymbol\\beta^{(i)}) - \\eta \\nabla_{(\\beta_0, \\boldsymbol\\beta)}\\mathcal{L}(\\beta_0^{(i)}, \\boldsymbol\\beta^{(i)} \\mid \\mathcal{X}^{(i)}, \\mathcal{Y}^{(i)}),\n",
    "$$\n",
    "\n",
    "where $\\eta>0$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the SGD algorithm\n",
    "\n",
    "iterations = 2000\n",
    "losses = []\n",
    "learning_rate = 0.001\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    x_batch, y_batch = next(train_data)\n",
    "    losses.append(mse_loss(x_batch, y_batch, beta0, beta))\n",
    "    beta0_grad, beta_grad = grad(x_batch, y_batch, beta0, beta)\n",
    "    beta0 -= learning_rate * beta0_grad\n",
    "    beta -= learning_rate * beta_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the learned regression function and loss values\n",
    "\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "fig.add_subplot(121)\n",
    "plt.scatter(x_sample, y_sample, label='Data samples')\n",
    "plt.plot(x_sample, true_beta0 + x_sample * true_beta, color='C1', label='True fn')\n",
    "plt.plot(x_sample, beta0 + x_sample * beta, color='C2', label='Initialised fn')\n",
    "plt.title(r'Final parameters: $\\beta_0$: {:.2f}, $\\beta_1$: {:.2f}. MSE loss: {:.4f}'.format(\n",
    "    np.squeeze(beta0), np.squeeze(beta), mse_loss(x_sample, y_sample, beta0, beta))\n",
    "         )\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(122)\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE loss\")\n",
    "plt.title(\"Loss vs iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. Does the solution above look reasonable?\n",
    "2. Play around with different values of the learning rate. How is the convergence of the algorithm affected?\n",
    "3. Try using different batch sizes and re-run the algorithm. What changes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multilayer Perceptron (MLP)\n",
    "\n",
    "The MLP is a type of neural network model, that composes multiple affine transformations together, and applying a pointwise nonlinearity in between:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol{h}^{(0)} &:= \\boldsymbol{x},\\\\\n",
    "\\boldsymbol{h}^{(k)} &= \\sigma\\left((\\boldsymbol{h}^{(k-1)})^T \\boldsymbol{W}^{(k-1)} + \\boldsymbol{b}^{(k-1)} \\right),\\qquad k=1,\\ldots, L,\\\\\n",
    "\\hat{\\boldsymbol{y}} &= \\sigma_{out}\\left((\\boldsymbol{h}^{(L)})^T \\boldsymbol{W}^{(L)} + \\boldsymbol{b}^{(L)} \\right),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol x\\in\\mathbb{R}^D$, $\\hat{\\boldsymbol{y}}\\in\\mathbb{R}^{n_{L+1}}$, $L$ is the number of hidden layers in the model, $\\boldsymbol{W}^{(k)}\\in\\mathbb{R}^{n_{k}\\times n_{k+1}}$, $\\boldsymbol{b}^{(k)}\\in\\mathbb{R}^{n_{k+1}}$, $\\boldsymbol{h}^{(k)}\\in\\mathbb{R}^{n_k}$, $\\sigma, \\sigma_{out}: \\mathbb{R}\\mapsto\\mathbb{R}$ are activation functions that operate element-wise, $n_k$ is the number of units in the $k$-th hidden layer, and we have set $n_0 := D$.\n",
    "\n",
    "For this section of the tutorial we will use the California Housing dataset from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "california_housing_data = fetch_california_housing()\n",
    "print(california_housing_data['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test splits\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(california_housing_data.data, \n",
    "                                                    california_housing_data.target,\n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the inputs\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should implement an MLP regression model in numpy. The model will have three hidden layers with 64 neurons each, and using a ReLU activation function. The final output layer will be a single neuron with no activation function to predict the target variable $y$.\n",
    "\n",
    "The building block of the model is the dense (or fully-connected) layer. The following function should implement the affine transformation of this layer, given kernel and bias parameters and the input to the layer. It should return the layer pre-activations (no activation function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDIT THIS FUNCTION\n",
    "def dense(x, W, b):\n",
    "    \n",
    "    # x: K x h_in array of inputs\n",
    "    # W: h_in x h_out array for kernel matrix parameters\n",
    "    # b: Length h_out 1-D array for bias parameters\n",
    "    # returns: K x h_out output array \n",
    "    \n",
    "    h = b + x @ W  ## <-- pre-activations\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden layers of our model will use a ReLU activation function. You should implement that in the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDIT THIS FUNCTION\n",
    "def relu(h):\n",
    "    \n",
    "    # h: K x h_out array of pre-activations\n",
    "    # returns: K x h_out output array \n",
    "    \n",
    "    h = np.maximum(h, 0.)  ## <-- post-activations\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our MLP will need the following parameters:\n",
    "\n",
    "Input layer -> first hidden layer:\n",
    "* Kernel $\\boldsymbol{W}^{(0)} \\in\\mathbb{R}^{8\\times 64}$\n",
    "* Bias $\\boldsymbol{b}^{(0)} \\in\\mathbb{R}^{64}$\n",
    "\n",
    "Hidden layer -> hidden layer:\n",
    "* Kernel $\\boldsymbol{W}^{(k)} \\in\\mathbb{R}^{64\\times 64}$, $k=1, 2$\n",
    "* Bias $\\boldsymbol{b}^{(k)} \\in\\mathbb{R}^{64}$, $k=1, 2$\n",
    "\n",
    "Hidden layer -> output layer:\n",
    "* Kernel $\\boldsymbol{W}^{(3)} \\in\\mathbb{R}^{64\\times 1}$\n",
    "* Bias $\\boldsymbol{b}^{(3)} \\in\\mathbb{R}^{1}$\n",
    "\n",
    "We will create these parameters as numpy arrays, and initialise the kernel values with samples from a zero-mean Gaussian distribution with variance $2 / (n_{in} + n_{out})$, where $n_{in}$ and $n_{out}$ are the number of neurons going in and out of the dense layer respectively. This initialisation strategy is known as [Glorot initialisation](http://proceedings.mlr.press/v9/glorot10a.html). The bias parameters will be initialised to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameters\n",
    "\n",
    "var0 = 2. / (64 + 8)\n",
    "W0 = np.random.randn(8, 64) * np.sqrt(var0)\n",
    "b0 = np.zeros(64)\n",
    "\n",
    "var1 = 2. / (64 + 64)\n",
    "W1 = np.random.randn(64, 64) * np.sqrt(var1)\n",
    "b1 = np.zeros(64)\n",
    "\n",
    "var2 = 2. / (64 + 64)\n",
    "W2 = np.random.randn(64, 64) * np.sqrt(var2)\n",
    "b2 = np.zeros(64)\n",
    "\n",
    "var3 = 2. / (1 + 64)\n",
    "W3 = np.random.randn(64, 1) * np.sqrt(var3)\n",
    "b3 = np.zeros(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should use these parameters and your `dense` function to create the MLP model. Remember that the hidden layers of the model should use a ReLU activation, and the output of the model should not use an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDIT THIS FUNCTION\n",
    "def mlp(x):\n",
    "    \n",
    "    # x: K x 8 array of inputs\n",
    "    # returns: K x 1 output array \n",
    "    \n",
    "    h = dense(x, W0, b0)\n",
    "    h = relu(h)\n",
    "    h = dense(h, W1, b1)\n",
    "    h = relu(h)\n",
    "    h = dense(h, W2, b2)\n",
    "    h = relu(h)\n",
    "    y = dense(h, W3, b3)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the output of your initialised model\n",
    "\n",
    "y_hat_test = mlp(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement the MLP in TensorFlow and train it on the dataset. The following function should build the same MLP model in TensorFlow, and set the parameter values to the same as you have used in the numpy implementation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDIT THIS FUNCTION\n",
    "def get_model():\n",
    "    \n",
    "    # This function should create the MLP and set its parameters to the same as above\n",
    "    # returns: TensorFlow Sequential model object\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(8,)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.layers[0].kernel.assign(W0)\n",
    "    model.layers[0].bias.assign(b0)\n",
    "    \n",
    "    model.layers[1].kernel.assign(W1)\n",
    "    model.layers[1].bias.assign(b1)\n",
    "    \n",
    "    model.layers[2].kernel.assign(W2)\n",
    "    model.layers[2].bias.assign(b2)\n",
    "    \n",
    "    model.layers[3].kernel.assign(W3)\n",
    "    model.layers[3].bias.assign(b3)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to get the TensorFlow MLP model\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the output is the same as for your numpy implementation\n",
    "\n",
    "tf_y_hat_test = model(x_test).numpy()\n",
    "np.allclose(tf_y_hat_test, y_hat_test, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train the TensorFlow MLP model. First, you should create Dataset objects for the training and test sets. \n",
    "\n",
    "* The Datasets should return a tuple of inputs and outputs\n",
    "* The training Dataset should be shuffled (use a buffer size of 1024)\n",
    "* Both Datasets should be batched (use a batch size of 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDIT THIS FUNCTION\n",
    "def get_datasets():\n",
    "    \n",
    "    # returns: A tuple of tf.data.Dataset objects for train and test sets\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    \n",
    "    train_dataset = train_dataset.shuffle(1024)\n",
    "    \n",
    "    train_dataset = train_dataset.batch(64)\n",
    "    test_dataset = test_dataset.batch(64)\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function to get the Datasets\n",
    "\n",
    "train_dataset, test_dataset = get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to compile and train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model\n",
    "\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "history = model.fit(train_dataset, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss vs epoch\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE loss\")\n",
    "plt.title(\"Loss vs epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test Dataset\n",
    "\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. How do the training and test losses above compare?\n",
    "2. Play around with different numbers of hidden layers and number of neurons in the MLP. How is the performance of the model affected?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
