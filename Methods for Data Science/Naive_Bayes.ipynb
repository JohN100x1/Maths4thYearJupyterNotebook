{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dY98Oclcl92-"
   },
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes is a classification algorithm based on Bayes' theorem. Bayesâ€™ theorem provides a way to calculate the probability of a data point belonging to a given class, given our prior knowledge. It is defined as\n",
    "\n",
    "$$\n",
    "\\mathbb P (class|data) = \\frac{\\mathbb P (data|class) \\ \\mathbb P (class)}{\\mathbb P (data)} ,\n",
    "$$\n",
    "\n",
    "where $\\mathbb P (class | data)$ is the probability over the potential classes given the provided data. The different probabilities $\\mathbb P$ you see in the equations above are commonly called prior, likelihood, evidence, and posterior as follows.\n",
    "\n",
    "$$\n",
    "\\overbrace{\\mathbb P (class|data)}^{\\text{posterior}} = \\frac{\\overbrace{\\mathbb P (data|class)}^{\\text{likelihood}} \\ \\overbrace{\\mathbb P (class)}^{\\text{prior}}}{\\underbrace{\\mathbb P (data)}_{\\text{evidence}}}\n",
    "$$\n",
    "\n",
    "The algorithm is 'naive', because of its assumption that features of data are independent given the class label. Let us call the data features $x_1, \\dots, x_i, \\dots, x_n$ and the class label $y$, and rewrite Bayes theorem in these terms:\n",
    "\n",
    "$$\n",
    "\\mathbb P (y|x_1, \\dots, x_n) = \\frac{\\mathbb P (x_1, \\dots, x_n|y) * \\mathbb P (y)}{\\mathbb P (x_1, \\dots, x_n)} \\, . \n",
    "$$\n",
    "\n",
    "Then, the naive assumption of conditional independence between any two features given the class label can be expressed as\n",
    "\n",
    "$$\n",
    "\\mathbb P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = \\mathbb P (x_i | y) \\, .\n",
    "$$\n",
    "\n",
    "For all $i$, we can simply Bayes' theorem to:\n",
    "\n",
    "$$\n",
    "\\mathbb P (y | x_1, \\dots, x_n) = \\frac{\\mathbb P (y) \\prod_{i=1}^n \\mathbb P(x_i | y)}{\\mathbb P (x_1, \\dots, x_n)} \\, .\n",
    "$$\n",
    "\n",
    "Since $\\mathbb P (x_1, \\dots, x_n)$ is the constant input, we can define the following proportional relationship\n",
    "\n",
    "$$\n",
    "\\mathbb P (y|x_1, \\dots, x_n) \\propto \\mathbb P (y) \\prod_{i=1}^n \\mathbb P(x_i | y) \\, ,\n",
    "$$\n",
    "\n",
    "and can use it to classify any data point as\n",
    "\n",
    "$$\n",
    "\\hat y = \\underset{y}{\\text{arg max}} \\ \\mathbb P (y) \\prod_{i=1}^n \\mathbb P(x_i | y) \\, .\n",
    "$$\n",
    "\n",
    "To learn how this algorithm works in practice, we define a simple data set of emails being either spam or not (adopted from Chapter 3.5, Exercise 3.22 in Machine Learning: A Probabilistic Perspective by Murphy). _Note that Naive Bayes can indeed be used for multiclass classification, however we use it here as a binary classifier._ \n",
    "\n",
    "We will work with the packages numpy and pandas, but also make our lives a bit easier with sklearn's implemented feature extractor to count words and its validation module to check whether data arrives in the format we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8r_Xo0HklZFx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Callable\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.utils.validation import check_X_y, check_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wbwhd07rsZxv"
   },
   "outputs": [],
   "source": [
    "def make_spam_dataset(show_X=True):\n",
    "    \"\"\" Create a small toy dataset for MultinomialNB implementation\n",
    "    \n",
    "    Returns:\n",
    "        X: word count matrix\n",
    "        y: indicator of whether or not message is spam\n",
    "        msg_tx_func: a function to transform new test data into word count matrix\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = [\n",
    "        'secret', 'offer', 'low', 'price', 'valued', 'customer', 'today',\n",
    "        'dollar', 'million', 'sports', 'is', 'for', 'play', 'healthy', 'pizza'\n",
    "    ]\n",
    "\n",
    "    spam = [\n",
    "        'million dollar offer',\n",
    "        'secret offer today',\n",
    "        'secret is secret'\n",
    "    ]\n",
    "    \n",
    "    not_spam = [\n",
    "        'low price for valued customer',\n",
    "        'play secret sports today',\n",
    "        'sports is healthy',\n",
    "        'low price pizza'\n",
    "    ]\n",
    "\n",
    "    all_messages = spam + not_spam\n",
    "    \n",
    "    vectorizer = CountVectorizer(vocabulary=vocab)\n",
    "    word_counts = vectorizer.fit_transform(all_messages).toarray()\n",
    "    df = pd.DataFrame(word_counts, columns=vocab)\n",
    "    is_spam = [1] * len(spam) + [0] * len(not_spam)  # storing our labels in a list (1 means spam email, 0 means no spam email)\n",
    "    msg_tx_func = lambda x: vectorizer.transform(x).toarray()\n",
    "    \n",
    "    if show_X:\n",
    "        display(df)\n",
    "        \n",
    "    return df.to_numpy(), np.array(is_spam), msg_tx_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LAF17qr2sv9G"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>secret</th>\n",
       "      <th>offer</th>\n",
       "      <th>low</th>\n",
       "      <th>price</th>\n",
       "      <th>valued</th>\n",
       "      <th>customer</th>\n",
       "      <th>today</th>\n",
       "      <th>dollar</th>\n",
       "      <th>million</th>\n",
       "      <th>sports</th>\n",
       "      <th>is</th>\n",
       "      <th>for</th>\n",
       "      <th>play</th>\n",
       "      <th>healthy</th>\n",
       "      <th>pizza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   secret  offer  low  price  valued  customer  today  dollar  million  \\\n",
       "0       0      1    0      0       0         0      0       1        1   \n",
       "1       1      1    0      0       0         0      1       0        0   \n",
       "2       2      0    0      0       0         0      0       0        0   \n",
       "3       0      0    1      1       1         1      0       0        0   \n",
       "4       1      0    0      0       0         0      1       0        0   \n",
       "5       0      0    0      0       0         0      0       0        0   \n",
       "6       0      0    1      1       0         0      0       0        0   \n",
       "\n",
       "   sports  is  for  play  healthy  pizza  \n",
       "0       0   0    0     0        0      0  \n",
       "1       0   0    0     0        0      0  \n",
       "2       0   1    0     0        0      0  \n",
       "3       0   0    1     0        0      0  \n",
       "4       1   0    0     1        0      0  \n",
       "5       1   1    0     0        1      0  \n",
       "6       0   0    0     0        0      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define our variables and print X\n",
    "X, y, tx_func = make_spam_dataset(show_X=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dKlchW7Eswez"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# see how y looks like\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KReuBEwe1hye"
   },
   "source": [
    "Next, we train the Naive Bayes classifier with a `train` function where we define the prior. Recall from our lectures that the prior is the probability distribution incorporating our knowledge of the data. Consequently, we use the available training set to define it. We do the first step for you to separate the training examples of both classes, but you need to define the binomial probability distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aq5Bwv4h1hZk"
   },
   "outputs": [],
   "source": [
    "# EDIT THIS FUNCTION\n",
    "def train(X, y):\n",
    "  \"\"\" Use training data for Naive Bayes classifier \"\"\"\n",
    "\n",
    "  # not strictly necessary, but this ensures we have clean input\n",
    "  X, y = check_X_y(X, y)\n",
    "  n = X.shape[0]\n",
    "\n",
    "  # reorder X as a 2-dimensional array; each dimension contains data examples of only one of our two classes\n",
    "  X_by_class = np.array([X[y==c] for c in np.unique(y)])\n",
    "  # define prior\n",
    "  prior = np.array([len(X_class)/n for X_class in X_by_class]) ## <-- EDIT THIS LINE (hint: use python's list comprehension)\n",
    "\n",
    "  # count words in each class\n",
    "  word_counts = np.array([sub_arr.sum(axis=0) for sub_arr in X_by_class])\n",
    "  # define likelihood\n",
    "  lk_word = word_counts / word_counts.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "  return prior, lk_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "M0A0YJje7rup"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior: [0.57142857 0.42857143]\n"
     ]
    }
   ],
   "source": [
    "# call function and print prior\n",
    "prior, lk_word = train(X, y)\n",
    "print('Prior:', prior)\n",
    "# Prior: the probability of class (not spam or spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar([\"Not spam\",\"Spam\"],height=prior)\n",
    "# The training set started with 3 spam emails and 4 non-spam emails, the prior is based on this proportion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPfkKXvk7oyC"
   },
   "source": [
    "#### Questions:\n",
    "1. Do you understand what these two values stand for? \n",
    "2. Plot them as a Bernoulli distribution and explain where the difference comes from.\n",
    "\n",
    "Now we can predict whether any given email is spam or not. Let us define first a function that computes the posterior and afterwards a second function that predicts the binary classification.\n",
    "\n",
    "_Hint: If the following cell includes too many and too complicated steps at once, print the output of each variable one line after the other._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "EYrQ6cx91hSA"
   },
   "outputs": [],
   "source": [
    "# EDIT THIS FUNCTION\n",
    "def predict_proba(X, y):\n",
    "  \"\"\" Predict probability of class \"\"\"\n",
    "\n",
    "  X = check_array(X)\n",
    "  X, y = check_X_y(X, y)\n",
    "\n",
    "  # insert train function within this function\n",
    "  prior, lk_word = train(X, y)\n",
    "\n",
    "  # loop over each observation to calculate conditional probabilities\n",
    "  class_numerators = np.zeros(shape=(X.shape[0], prior.shape[0]))\n",
    "  for i, x in enumerate(X):\n",
    "    \n",
    "    # count how often words appear in each email\n",
    "    word_exists = x.astype(bool)\n",
    "    #print(word_exists)\n",
    "    \n",
    "    # compute likelihoods of words (probability of data appearing in any class)\n",
    "    lk_words_present = lk_word[:, word_exists] ** x[word_exists]\n",
    "    #print(lk_words_present)\n",
    "    \n",
    "    # compute likelihood of entire message with likelihoods of words\n",
    "    lk_message = (lk_words_present).prod(axis=1)\n",
    "    #print(lk_message)\n",
    "    \n",
    "    # combine likelihood and prior to numerator\n",
    "    class_numerators[i] = np.multiply(prior,lk_message)  ## <-- SOLUTION\n",
    "    \n",
    "  normalize_term = class_numerators.sum(axis=1).reshape(-1, 1)\n",
    "  posteriors = class_numerators / normalize_term\n",
    "  if not (posteriors.sum(axis=1) - 1 < 0.0001).all():\n",
    "    raise ValueError('Rows should sum to 1')\n",
    "  return posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "_ViH8LlA1hNn"
   },
   "outputs": [],
   "source": [
    "posteriors = predict_proba(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUJxJ9h6C4g_"
   },
   "source": [
    "Now, we can predict in a binary fashion by asserting any data points to the class with the highest probability. Here, we take our emails we trained our Naive Bayes classifier on also to evaluate it, but the evaluation normally happens on unseen emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "36IsDfzH1eXN"
   },
   "outputs": [],
   "source": [
    "def predict(X, y):\n",
    "  \"\"\" Predict class with highest probability \"\"\"\n",
    "  return predict_proba(X, y).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "FroLvChb1hKg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0 0]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "preds = predict(X, y)\n",
    "print(preds)\n",
    "print(f'Accuracy: {(preds==y).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHJr2ZflEFGp"
   },
   "source": [
    "#### Questions:\n",
    "1. Define your own three short emails as a test set and evaluate our Naive Bayes classifier on it without re-training it on them. What do you observe? \n",
    "2. What words have you included in emails of the test set that make them being classified as spam or not spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "NbFBjsD71ee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0 0]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "test_X = np.array([[2,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "                  [0,0,0,0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                  [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1],\n",
    "                  [0,0,1,2,0,0,0,0,0,1,1,0,0,0,0]])\n",
    "test_y = np.array([1, 1, 0, 0])\n",
    "test_preds = predict(X, y)\n",
    "print(test_preds)\n",
    "print(f'Accuracy: {(test_preds==y).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Smp9IiWMzzbv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPMbKP8JteVd7yjb6yNz3L8",
   "collapsed_sections": [],
   "name": "Naive_Bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
