{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for Data Science\n",
    "### Deep Learning / Neural Networks and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "[1. Introduction](#introduction)\n",
    "\n",
    "[2. TensorFlow Tensors and Variables](#tensors_and_variables)\n",
    "\n",
    "[3. The Sequential class](#sequential)\n",
    "\n",
    "[4. The tf.data module](#tf.data)\n",
    "\n",
    "[5. TensorFlow regularisers, Dropout layers and callbacks](#tf_regularisation)\n",
    "\n",
    "[6. CNNs and feature maps](#cnnsfeaturemaps)\n",
    "\n",
    "[References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"introduction\"></a>\n",
    "## Introduction\n",
    "\n",
    "Welcome to the deep learning / neural networks section of the Methods for Data Science module! \n",
    "\n",
    "In this section of the course, you will learn the fundamentals of deep learning models, as well as techniques for how to train, regularise and validate them. \n",
    "\n",
    "We will cover widespread deep learning architectures such as the multilayer perceptron (MLP) and convolutional neural network (CNN), with a focus on understanding the mathematical operations and transformations included in these models. We will also look at several popular network optimisation algorithms, as well as the important error backpropagation algorithm, which is central to the training of neural networks. Regularisation techniques covered are weight regularisation, early stopping, and dropout. \n",
    "\n",
    "The video content for this material is split into two types. There are standard 'lecture-style' videos, where the core material and theory behind deep learning models is presented, and then there are 'coding tutorial' videos, where you will learn to implement these concepts and ideas in the deep learning framework TensorFlow.\n",
    "\n",
    "TensorFlow is an open source software library used for machine learning applications, especially deep learning. It uses symbolic mathematics (instead of purely numerical computations), which enables it to perform operations like automatic differentiation on a computational graph such as a neural network. Another major benefit is its ability to perform computations on GPU hardware, potentially leading to large speedups. \n",
    "\n",
    "This notebook contains many blank code cells in the sections listed above. The coding tutorial videos will step through the different parts of the TensorFlow library, and show you how to fill in these code cells. The idea is that you should follow along with these videos and code in all the examples yourself. This way, you will gain familiarity in how to use TensorFlow, and you should feel free to pause the video and try things out for yourself to gain a deeper understanding.\n",
    "\n",
    "Throughout these coding tutorials, it is a good idea to use the [documentation](https://www.tensorflow.org/api_docs/python/tf) as a regular reference for the various functions and classes that we will be looking at. \n",
    "\n",
    "You will be able to run this notebook and follow the examples from the coding tutorial videos within the Anaconda environment you have installed for TensorFlow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"tensors_and_variables\"></a>\n",
    "## TensorFlow Tensors and Variables\n",
    "\n",
    "In this section we will introduce some fundamental building blocks and operations in TensorFlow. [Tensors](https://www.tensorflow.org/api_docs/python/tf/Tensor) and [Variables](https://www.tensorflow.org/api_docs/python/tf/Variable) are low-level objects that we will be using all the time in TensorFlow.\n",
    "\n",
    "#### Tensors\n",
    "You can think of Tensors as being multidimensional versions of vectors and arrays. Of course, these are the objects that Tensorflow gets its name from. When we build our neural network models, what we’re doing is defining a computational graph, where input data is processed through the layers of the network and sent through the graph all the way to the outputs. Tensors are the objects that get passed around within the graph, and capture those computations within the graph. \n",
    "\n",
    "Let’s take a look at some examples to get a better feel for how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a constant Tensor\n",
    "\n",
    "a = tf.constant([1, 2, 3])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Tensors have `shape` and `dtype` properties, similar to NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine shape property\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine dtype property\n",
    "\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor objects can have different types, just like NumPy arrays. Take a look [here](https://www.tensorflow.org/api_docs/python/tf) for a complete list of available types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensor objects of different type\n",
    "\n",
    "string_tensor = tf.constant([\"Hello world!\"], tf.string)\n",
    "float_tensor  = tf.constant([3.14159, 2.71828], tf.float32)\n",
    "print(string_tensor)\n",
    "print(float_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a rank-2 Tensor \n",
    "\n",
    "b = tf.constant([[1.2, 0.4, 0.7], [-9.3, 4.5, 1.1]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Tensor rank\n",
    "\n",
    "tf.rank(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tensor with tf.ones\n",
    "\n",
    "tf.ones((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tensor with tf.zeros\n",
    "\n",
    "tf.zeros((3,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert a TensorFlow Tensor into a NumPy array using the `numpy` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Tensor to NumPy array\n",
    "\n",
    "b_np = b.numpy()\n",
    "print(type(b_np))\n",
    "b_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute Tensor multiplication using `tf.tensordot` (see the [docs](https://www.tensorflow.org/api_docs/python/tf/tensordot)). The `axes` argument can be an integer or list of integers. When it is a single integer `n`, the contraction is performed over the last `n` axes of the first Tensor and the first `n` axes of the second Tensor. If it is a list, then the elements of the list specify the axes to contract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute matrix-vector product\n",
    "\n",
    "# tf.tensordot(b, a, axes=1)  # Type error\n",
    "\n",
    "a = tf.cast(a, tf.float32)\n",
    "tf.tensordot(b, a, axes=1)  # Sum over last axis of b and first axis of a\n",
    "tf.tensordot(b, a, axes=[[1], [0]])  # Equivalent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of two rank-2 Tensors, we can use the `tf.linalg.matmul` function. (In fact, we can use rank >= 2 Tensors with `tf.linalg.matmul` - see the [docs](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul).) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf.linalg.matmul to compute product\n",
    "\n",
    "# tf.linalg.matmul(b, a)  # Shape error\n",
    "\n",
    "print(b.shape)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful operations to manipulate Tensor shapes are `tf.expand_dims`, `tf.squeeze` and `tf.reshape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an extra dimension to a Tensor\n",
    "\n",
    "a = tf.expand_dims(a, 1)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf.matmul, tf.squeeze and tf.reshape\n",
    "\n",
    "# tf.linalg.matmul(b, a)\n",
    "tf.reshape(tf.squeeze(tf.linalg.matmul(b, a)), [1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also often useful to fill Tensors with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random normal Tensor\n",
    "\n",
    "tf.random.normal((3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random integer Tensor\n",
    "\n",
    "tf.random.uniform(shape=(2, 4), minval=0, maxval=10, dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### McCulloch-Pitts neuron\n",
    "As an example, we will use Tensors to implement the McCulloch-Pitts neuron for a simple logical function. The McCulloch-Pitts neuron operates on boolean inputs, and uses a threshold activation to produce a boolean output. The function can be written as\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \n",
    "\\begin{cases}\n",
    "1 \\quad \\text{if } \\sum_i x_i \\ge b\\\\\n",
    "0 \\quad \\text{if } \\sum_i x_i < b\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AND function\n",
    "\n",
    "def logical_and(x):\n",
    "    return tf.cast(tf.math.greater_equal(tf.reduce_sum(x), tf.reduce_sum(tf.shape(x))), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the AND function with a few examples\n",
    "\n",
    "logical_and(tf.constant([1, 1]))\n",
    "logical_and(tf.constant([1, 1, 0]))\n",
    "logical_and(tf.ones((2, 3), dtype=tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the OR function\n",
    "\n",
    "def logical_or(x):\n",
    "    return tf.cast(tf.math.greater_equal(tf.reduce_sum(x), 1), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the OR function with a few examples\n",
    "\n",
    "logical_or(tf.constant([1, 0]))\n",
    "logical_or(tf.zeros(3,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise.* Define the function for the NOR operation below (all inputs must be zero) for inputs `x`. *Hint: use the* `tf.math.logical_not` *function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NOR function\n",
    "\n",
    "def logical_nor(x):\n",
    "    return tf.math.logical_not(logical_or(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the NOR function with a few examples\n",
    "\n",
    "print(logical_nor(tf.constant([1, 0])))  # False\n",
    "print(logical_nor(tf.constant([0, 0])))  # True\n",
    "print(logical_nor(tf.constant([0, 0, 0])))  # True\n",
    "print(logical_nor(tf.constant([1, 0, 1])))  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables\n",
    "Tensors are *immutable objects*; that is, their state cannot be modified. The operations they encapsulate (or the values of a constant Tensor) are fixed. Variables are special kinds of Tensors that have *mutable state*, so their values can be updated. This is useful for parameters of a model, such as the weights and biases in a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorFlow Variable\n",
    "\n",
    "initial_value = tf.random.normal((2, 2))\n",
    "u = tf.Variable(initial_value)\n",
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks very similar to a Tensor. However, Variables come with extra methods for updating their state, such as `assign`, `assign_add` and `assign_sub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a new value to the Variable\n",
    "\n",
    "new_value = 2. * tf.ones((2, 2))\n",
    "u.assign(new_value)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a value to the Variable\n",
    "\n",
    "increment = tf.constant([[0., 0.], [1., 1.]])\n",
    "u.assign_add(increment)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract a value from the Variable\n",
    "\n",
    "decrement = tf.constant([[2., 0.], [2., 0.]])\n",
    "u.assign_sub(decrement)\n",
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will often use Variables in operations within the computational graph. The result of the operation is a Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a Variable in a simple operation\n",
    "\n",
    "v = tf.Variable([2.6, -0.4])\n",
    "s = v + 1\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The perceptron\n",
    "The perceptron is also a linear binary classifier, but with more flexible weights. It can be written as the following function\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \n",
    "\\begin{cases}\n",
    "1 \\quad \\text{if } \\sum_i w_i x_i + b \\ge 0\\\\\n",
    "0 \\quad \\text{if } \\sum_i w_i x_i + b < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "As an example, we will use Tensors and Variables to implement the perceptron classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the weights and bias as Variables\n",
    "\n",
    "weights = tf.Variable(tf.constant([1., 0.5]), name='weights')\n",
    "bias = tf.Variable(tf.constant(-0.5), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the perceptron classifier\n",
    "\n",
    "def perceptron(x):\n",
    "    return tf.math.greater_equal(tf.tensordot(x, weights, axes=1) + bias, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random set of test points\n",
    "\n",
    "x = tf.random.normal((100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the points coloured by class prediction\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "preds = perceptron(x)\n",
    "# print(preds)\n",
    "positive_class = x[preds]\n",
    "negative_class = x[~preds]\n",
    "plt.scatter(positive_class[:, 0], positive_class[:, 1], alpha=0.5, label='positive')\n",
    "plt.scatter(negative_class[:, 0], negative_class[:, 1], alpha=0.5, label='negative')\n",
    "plt.title(\"Perceptron classifications\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Update the weights and bias and re-plot\n",
    "\n",
    "weights.assign_sub(tf.constant([1.5, 0.3]))\n",
    "bias.assign_add(tf.constant(0.5))\n",
    "\n",
    "preds = perceptron(x)\n",
    "positive_class = x[preds]\n",
    "negative_class = x[~preds]\n",
    "plt.scatter(positive_class[:, 0], positive_class[:, 1], alpha=0.5, label='positive')\n",
    "plt.scatter(negative_class[:, 0], negative_class[:, 1], alpha=0.5, label='negative')\n",
    "plt.title(\"Perceptron classifications\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise.* Can you find weights and bias values to implement the NOT gate for $x\\in\\{0, 1\\}$ and the XOR gate for $x\\in\\{0, 1\\}^2$? If yes, what are the values? If no, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"sequential\"></a>\n",
    "## The Sequential class\n",
    "\n",
    "There are multiple ways to build and apply deep learning models in Tensorflow, from high-level, quick and easy-to-use APIs, to low-level operations. In this section you will walk through the high-level Keras API for quickly building, training, evaluating and predicting from deep learning models. In particular, you will see how to use the `Sequential` class to implement MLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `Dense` layer\n",
    "\n",
    "We will see how to build MLP models using the `Dense` layer class from TensorFlow. This class implements the layer transformation $\n",
    "\\mathbf{h}^{(k+1)} = \\sigma\\left( \\mathbf{W}^{(k)}\\mathbf{h}^{(k)} + \\mathbf{b}^{(k)} \\right)\n",
    "$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dense layer\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "dense_layer = Dense(4, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the layer parameters\n",
    "\n",
    "dense_layer.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow models are designed to process batches of data at once, and always expect inputs to have a batch dimension in the first axis. For example, a batch of 16 inputs, each of which is a length 4 vector, should have a shape `[16, 4]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the dense layer on an input to create the weights\n",
    "\n",
    "x = tf.ones((2, 6))\n",
    "y = dense_layer(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the layer parameters\n",
    "\n",
    "dense_layer.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the parameters of the layer are Variable objects. This makes sense, as recall that Variables are mutable, and we will want to modify them during network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP model\n",
    "\n",
    "To construct an MLP model, we stack multiple `Dense` layers together by passing them in a list to the `Sequential` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an MLP model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "mlp = Sequential([\n",
    "    Dense(4, activation='relu'), # , input_shape=(6,))\n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default value for the `activation` keyword argument is `None`, in which case no activation (linear activation) is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model on an input to create the weights\n",
    "\n",
    "x = tf.random.normal((2, 6))\n",
    "y = mlp(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth knowing that the `Sequential` class itself inherits from the `Layer` class, so all the same properties and methods are also available for `Sequential` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the model parameters\n",
    "\n",
    "mlp.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the model layers\n",
    "\n",
    "# mlp.layers\n",
    "# mlp.layers[1]\n",
    "mlp.layers[1].kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Sequential` models (and layers) also have `trainable_weights` and `non_trainable_weights` properties, as weights (Variables) that are created can be set to trainable or non-trainable.\n",
    "\n",
    "#### Train an MLP model on the MNIST dataset\n",
    "Multidimensional inputs (i.e., with rank >= 2) can also be processed by an MLP network by simply unrolling, or flattening the dimensions. This can be done easily using the `Flatten` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm ~/.keras/datasets/mnist.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several datasets are available to load using the Keras API, see [the docs](https://www.tensorflow.org/api_docs/python/tf/keras/datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data shapes\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a few training data examples\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_rows, n_cols = 3, 5\n",
    "random_inx = np.random.choice(x_train.shape[0], n_rows * n_cols, replace=False)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 8))\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.1)\n",
    "\n",
    "for n, i in enumerate(random_inx):\n",
    "    row = n // n_cols\n",
    "    col = n % n_cols\n",
    "    axes[row, col].imshow(x_train[i])\n",
    "    axes[row, col].get_xaxis().set_visible(False)\n",
    "    axes[row, col].get_yaxis().set_visible(False)\n",
    "    axes[row, col].text(10., -1.5, f'Digit {y_train[i]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an MNIST classifier model\n",
    "\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "mnist_model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(64, activation='tanh'),\n",
    "    Dense(64, activation='tanh'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "mnist_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we need to specify a loss function to minimise, and an optimisation algorithm. The average negative log-likelihood on the training set is given by the categorical cross entropy\n",
    "\n",
    "$$\n",
    "L(\\theta) = -\\frac{1}{|\\mathcal{D}_{train}|} \\sum_{x_i\\in\\mathcal{D}_{train}}\\sum_{j=1}^{10} \\tilde{y}_{ij} \\ln f_\\theta(x_i)_j,\n",
    "$$\n",
    "\n",
    "where $f_\\theta$ is the neural network function (with parameters $\\theta$) that outputs a length 10 probability vector $f_\\theta(x_i)\\in\\mathbb{R}^{10}$ for an input example image $x_i\\in\\mathbb{R}^{28\\times 28}$, and $\\tilde{y}_{ij}$ is 1 if the correct label for example $i$ is $j$, and 0 otherwise.\n",
    "\n",
    "As our labels `y_train` and `y_test` are in sparse form, we use the `sparse_categorical_crossentropy` loss function. We also will use the stochastic gradient descent (SGD) optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "# NOTE: enter a couple of other example loss functions, including mse\n",
    "mnist_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image data is filled with integer pixel values from 0 to 255. To facilitate the training, we rescale the values to the interval $[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale the image data\n",
    "\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# mnist_model.fit(x_train, y_train, epochs=10, batch_size=64)\n",
    "history = mnist_model.fit(x_train, y_train, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross entropy loss\")\n",
    "plt.title(\"Training Loss vs epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "\n",
    "mnist_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from model\n",
    "\n",
    "preds = mnist_model.predict(x_test)\n",
    "# preds = mnist_model(x_test)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some predicted categorical distributions\n",
    "\n",
    "num_test_images = x_test.shape[0]\n",
    "\n",
    "random_inx = np.random.choice(num_test_images, 4)\n",
    "random_preds = preds[random_inx, ...]\n",
    "random_test_images = x_test[random_inx, ...]\n",
    "random_test_labels = y_test[random_inx, ...]\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 12))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=-0.2)\n",
    "\n",
    "for i, (prediction, image, label) in enumerate(zip(random_preds, random_test_images, random_test_labels)):\n",
    "    axes[i, 0].imshow(np.squeeze(image))\n",
    "    axes[i, 0].get_xaxis().set_visible(False)\n",
    "    axes[i, 0].get_yaxis().set_visible(False)\n",
    "    axes[i, 0].text(10., -1.5, f'Digit {label}')\n",
    "    axes[i, 1].bar(np.arange(len(prediction)), prediction)\n",
    "    axes[i, 1].set_xticks(np.arange(len(prediction)))\n",
    "    axes[i, 1].set_title(f\"Categorical distribution. Model prediction: {np.argmax(prediction)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise.* The MNIST dataset is an easy dataset, and the above model is far from optimal. Try experimenting with longer training times and/or model architecture changes to see if you can improve on the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"tf.data\"></a>\n",
    "## The `tf.data` module\n",
    "\n",
    "In this section we will introduce a standard data processing pipeline in TensorFlow, using the `tf.data` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Fashion-MNIST dataset\n",
    "We will build a deep learning classifier on the Fashion-MNIST dataset to demonstrate the use of the `tf.data` module. First we load the dataset using the Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion-MNIST dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r ~/.keras/datasets/fashion-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels\n",
    "\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a few training data examples\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_rows, n_cols = 3, 5\n",
    "random_inx = np.random.choice(x_train.shape[0], n_rows * n_cols, replace=False)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 8))\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.1)\n",
    "\n",
    "for n, i in enumerate(random_inx):\n",
    "    row = n // n_cols\n",
    "    col = n % n_cols\n",
    "    axes[row, col].imshow(x_train[i])\n",
    "    axes[row, col].get_xaxis().set_visible(False)\n",
    "    axes[row, col].get_yaxis().set_visible(False)\n",
    "    axes[row, col].text(10., -1.5, f'{classes[y_train[i]]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "fashion_mnist_model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10)\n",
    "], name='fashion_mnist_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "\n",
    "fashion_mnist_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main class that we will be working with is the `Dataset` class from the `tf.data` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into tf.data.Dataset objects\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) # pause after train_dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the Dataset object\n",
    "\n",
    "for inputs, labels in train_dataset.take(2):\n",
    "    print(type(inputs))\n",
    "    print(type(labels))\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset` objects come with `map` and `filter` methods for data preprocessing on the fly. For example, we can normalise the pixel values to the range $[0, 1]$ with the `map` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the pixel values\n",
    "\n",
    "def normalise_pixels(image, label):\n",
    "    return (tf.cast(image, tf.float32) / 255., label)  # Maybe add the tf.cast after the error\n",
    "\n",
    "train_dataset = train_dataset.map(normalise_pixels)\n",
    "test_dataset = test_dataset.map(normalise_pixels)\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also filter out data examples according to some criterion with the `filter` method. For example, if we wanted to exclude all data examples with label $9$ from the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all examples with label 9 (ankle boot)\n",
    "\n",
    "train_dataset = train_dataset.filter(lambda x, y: tf.math.logical_not(tf.equal(y, 9)))\n",
    "test_dataset = test_dataset.filter(lambda x, y: tf.math.logical_not(tf.equal(y, 9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the training dataset\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch the datasets\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = train_dataset.batch(batch_size)  # drop_remainder=True\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the element_spec\n",
    "\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.005)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "fashion_mnist_model.compile(optimizer=sgd, loss=loss_fn, metrics=['accuracy'])\n",
    "history = fashion_mnist_model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(17, 6))\n",
    "fig.add_subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Categorical cross entropy loss\")\n",
    "plt.title(\"Training Loss vs epoch\")\n",
    "fig.add_subplot(122)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Categorical accuracy\")\n",
    "plt.title(\"Training Accuracy vs epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "\n",
    "fashion_mnist_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from model\n",
    "\n",
    "for images, labels in test_dataset.take(1):\n",
    "    preds = fashion_mnist_model.predict(images)\n",
    "    preds = tf.nn.softmax(preds, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot some predicted categorical distributions\n",
    "\n",
    "num_test_images = preds.shape[0]\n",
    "\n",
    "random_inx = np.random.choice(num_test_images, 4)\n",
    "random_preds = preds[random_inx, ...]\n",
    "random_test_images = images.numpy()[random_inx, ...]\n",
    "random_test_labels = labels.numpy()[random_inx, ...]\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 12))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=-0.2)\n",
    "\n",
    "for i, (prediction, image, label) in enumerate(zip(random_preds, random_test_images, random_test_labels)):\n",
    "    axes[i, 0].imshow(np.squeeze(image))\n",
    "    axes[i, 0].get_xaxis().set_visible(False)\n",
    "    axes[i, 0].get_yaxis().set_visible(False)\n",
    "    axes[i, 0].text(10., -1.5, f'{classes[label]}')\n",
    "    axes[i, 1].bar(np.arange(len(prediction)), prediction)\n",
    "    axes[i, 1].set_xticks(np.arange(len(prediction)))\n",
    "    axes[i, 1].set_title(f\"Categorical distribution. Model prediction: {classes[np.argmax(prediction)]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise._ Rewrite the model to make it a binary classifier, and change the dataset processing steps above, to map 'Sandal', 'Sneaker' and 'Ankle boot' to a single label 0, and all other categories to label 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"tf_regularisation\"></a>\n",
    "## TensorFlow regularisers, Dropout layers and callbacks\n",
    "\n",
    "In this section we will build on what we have covered already with the `Sequential` API, and include weight regularisers, `Dropout` layers, and introduce callback objects - these are very useful objects for dynamically performing operations during the training run. An example is the `EarlyStopping` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we will use the diabetes dataset from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes_dataset = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataset description\n",
    "\n",
    "print(diabetes_dataset[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input and target data\n",
    "\n",
    "print(diabetes_dataset.keys())  # Run this first\n",
    "data = diabetes_dataset[\"data\"]\n",
    "targets = diabetes_dataset[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the target data (this will make clearer training curves)\n",
    "\n",
    "targets = (targets - targets.mean()) / targets.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the data into training and validation sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data, train_targets, val_targets = train_test_split(data, targets, test_size=0.2)  # Type train_test_split first\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(train_targets.shape)\n",
    "print(val_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into training, validation and test Dataset objects\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_targets))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_targets))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(353)\n",
    "\n",
    "train_dataset = train_dataset.batch(128)\n",
    "val_dataset = val_dataset.batch(89)\n",
    "\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the MLP model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation=\"relu\", input_shape=(train_data.shape[-1],)),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, including validation\n",
    "\n",
    "history = model.fit(train_dataset, epochs=100, validation_data=val_dataset, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularise the model\n",
    "\n",
    "Both $\\mathcal{l}^2$ and $\\mathcal{l}^1$ regularisation can easily be included using the `kernel_regularizer` and `bias_regularizer` keyword arguments in the `Dense` layer.\n",
    "\n",
    "Dropout can also be easily included as an additional layer of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the model using l2 regularisation and dropout\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "l2_coeff = 1e-5 # 1e-5\n",
    "rate = 0.5 # 0.3\n",
    "\n",
    "def get_regularised_model():\n",
    "    model = Sequential([\n",
    "        Dense(256, kernel_regularizer=regularizers.l2(l2_coeff), activation=\"relu\", input_shape=(train_data.shape[-1],)),\n",
    "        Dropout(rate),\n",
    "        Dense(256, kernel_regularizer=regularizers.l2(l2_coeff), activation=\"relu\"),\n",
    "        Dropout(rate),\n",
    "        Dense(256, kernel_regularizer=regularizers.l2(l2_coeff), activation=\"relu\"),\n",
    "        Dropout(rate),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "model = get_regularised_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, including validation\n",
    "\n",
    "history = model.fit(train_dataset, epochs=100, validation_data=val_dataset, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\mathcal{l}^2$ regularisation and dropout have helped to reduce the overfitting of the model. \n",
    "\n",
    "\n",
    "#### Callbacks\n",
    "We can go one step further and introduce early stopping as well, and save the model weights at the best validation score. We can do this with callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "\n",
    "model = get_regularised_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "# model.compile(optimizer='adam', loss=\"mse\") \n",
    "model.compile(optimizer='adam', loss=\"mse\", metrics=['mae']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `EarlyStopping` callback is a built-in callback in the `tf.keras.callbacks` module. You can see a complete list of built-in callbacks [here](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an EarlyStopping callback\n",
    "\n",
    "# earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_mae', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, including validation\n",
    "\n",
    "history = model.fit(train_dataset, epochs=100, validation_data=val_dataset, verbose=False,\n",
    "                   callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation metrics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['val_mae']) # Added in a second pass\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xticks(np.arange(len(history.history['loss'])))\n",
    "# plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.legend(['Training', 'Val loss', 'Val MAE'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise._ Take a look at some more of the callbacks available in the [callbacks module](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks) in TensorFlow, and have a go at implemented some of them in your model here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"cnnsfeaturemaps\"></a>\n",
    "## CNNs and feature maps\n",
    "\n",
    "In this section we will use the `Conv2D` and `MaxPool2D` layer to implement the convolution and pooling operations described above, and see how these easily fits into our existing model-building workflow.\n",
    "\n",
    "We will also see the effect of different kernel tensor choices on the output feature maps, and look at more complex feature maps from a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Conv2D` and `MaxPool2D` classes are imported from the `tf.keras.layers` module just as the `Flatten` and `Dense` layers we have already worked with. Note that there are also 1-D and 3-D variants of these layers available, which both work in a similar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dummy model with Conv2D and MaxPool2D layers\n",
    "\n",
    "from tensorflow.keras import Sequential # pause\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D # pause before import\n",
    "# pause\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(8, (3, 5), activation='relu', input_shape=(32, 32, 3)),  # pause before Conv2D, and after each arg\n",
    "    MaxPool2D((2, 2)), # pause before arg\n",
    "    Conv2D(16, 3, activation='relu'),  # pause after each arg\n",
    "    MaxPool2D(2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the layer variables' shapes\n",
    "\n",
    "print(model.layers[0].kernel.shape)\n",
    "print(model.layers[0].bias.shape)\n",
    "\n",
    "# Add afterwards\n",
    "print(model.layers[2].kernel.shape)\n",
    "print(model.layers[2].bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge detection filters\n",
    "The kernels (or filters) in CNNs are typically learned with backpropagation. However, simple low-level features such as edge detection kernels can also be designed by hand. In this section we will see the output of such low-level kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model with a Conv2D layer\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(1, (3, 3), activation=None, use_bias=False, input_shape=(None, None, 1))  # slight pauses in between args\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A shape dimension of `None` indicates that the model can take flexible input sizes in this dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the model's weights\n",
    "\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image as grayscale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = tf.io.read_file(\"./figures/oscar.png\")\n",
    "image = tf.io.decode_png(image, channels=1)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple and intuitive edge detection kernel is the [Sobel operator](https://en.wikipedia.org/wiki/Sobel_operator):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple edge detection filters\n",
    "\n",
    "sobel_x = tf.constant([[1, 0, -1], [2, 0, -2], [1, 0, -1]], dtype=tf.float32)\n",
    "sobel_y = tf.constant([[1, 2, 1], [0, 0, 0], [-1, -2, -1]], dtype=tf.float32)\n",
    "\n",
    "print(sobel_x)\n",
    "# print(sobel_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model kernel\n",
    "\n",
    "def assign_filter(arr):\n",
    "    model.weights[0].assign(arr[:, :, tf.newaxis, tf.newaxis])  # first just write arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the feature maps\n",
    "\n",
    "assign_filter(sobel_x)\n",
    "gx = model(image[None, ...])[0]  # Maybe run without the None (error), and then without the [0] first\n",
    "\n",
    "# Add this after running the above\n",
    "assign_filter(sobel_y)\n",
    "gy = model(image[None, ...])[0]\n",
    "\n",
    "g = tf.sqrt(tf.square(gx) + tf.square(gy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the image and feature map\n",
    "\n",
    "fig = plt.figure(figsize=(17, 6))\n",
    "fig.add_subplot(121)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "fig.add_subplot(122)\n",
    "plt.imshow(g, cmap='gray')  # First gx, then gy, then g\n",
    "plt.axis('off')\n",
    "plt.show()  # After executing, show the forehead markings with the cursor (after both gx and gy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract learned features from a pre-trained model\n",
    "In this section we will load a CNN model that has been pre-trained on the [ImageNet](http://www.image-net.org) dataset, which is a large scale image classification dataset which to date has over 20,000 categories and over 14 million images. Large deep learning models trained on this dataset tend to learn general, useful representations of image features that can be used for a range of image processing tasks.\n",
    "\n",
    "Below we will load the VGG-19 model ([Simonyan & Zisserman 2015](#Simonyan15)), which is available to load as a pre-trained model in the [`tf.keras.applications`](https://www.tensorflow.org/api_docs/python/tf/keras/applications) module. This might take a minute or two to download the first time you run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG-19 model\n",
    "\n",
    "# vgg = tf.keras.applications.VGG19(weights='imagenet')  # pause after vgg, applications\n",
    "vgg = tf.keras.applications.VGG19(weights='imagenet', include_top=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "\n",
    "vgg.summary()  # pause, slowly scroll to the bottom, then later to the top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will visualise the features extracted by this model at different levels of hierarchy for the following image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a colour image\n",
    "\n",
    "image = tf.io.read_file(\"./figures/hoover_dam.JPEG\")\n",
    "image = tf.io.decode_jpeg(image, channels=3)\n",
    "plt.figure(figsize=(6, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [functional API](https://www.tensorflow.org/guide/keras/functional) to create a multi-output model that outputs different hidden layer outputs within the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the multi-output model\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inputs = vgg.inputs\n",
    "layer_names = ['block1_conv2', 'block2_conv2', 'block3_conv4', 'block4_conv4', 'block5_conv4']\n",
    "outputs = [vgg.get_layer(layer_name).output for layer_name in layer_names] # add .output at the end\n",
    "vgg_features = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the model inputs and outputs Tensors\n",
    "\n",
    "vgg_features.inputs # inputs, then inputs, then vgg.inputs (then delete this)\n",
    "vgg_features.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the hierarchical features for this image\n",
    "\n",
    "image_processed = tf.keras.applications.vgg19.preprocess_input(image)\n",
    "features = vgg_features(image_processed[tf.newaxis, ...]) # pause after vgg_features and image_processed\n",
    "features = [image] + features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualise the features\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "n_rows, n_cols = 2, 3\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 14))\n",
    "fig.subplots_adjust(hspace=0.05, wspace=0.2)\n",
    "\n",
    "for i in range(len(features)):\n",
    "    feature_map = features[i]\n",
    "    num_channels = feature_map.shape[-1]\n",
    "    row = i // n_cols\n",
    "    col = i % n_cols\n",
    "    if i == 0:\n",
    "        axes[row, col].imshow(image)\n",
    "        axes[row, col].set_title('Original image')\n",
    "    else:\n",
    "        random_feature = np.random.choice(num_channels)\n",
    "        axes[row, col].imshow(feature_map[0, ..., random_feature])\n",
    "        axes[row, col].set_title('{}, channel {} of {}'.format(layer_names[i-1], random_feature + 1, num_channels))\n",
    "        \n",
    "    axes[row, col].get_xaxis().set_visible(False)\n",
    "    axes[row, col].get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise:* load one of your own images to view the features extracted by the VGG-19 network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"references\"></a>\n",
    "### References\n",
    "\n",
    "* Chen, J. & Kyrillidis, A., (2019), \"Decaying Momentum Helps Neural Network Training\", arXiv preprint arXiv:1910.04952.\n",
    "* Duchi, J., Hazan, E., & Singer, Y. (2011), \"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization\", *Journal of Machine Learning Research*, **12**, 2121–2159.\n",
    "* Dumoulin, V. & Visin, F. (2016), \"A guide to convolution arithmetic for deep learning\", arXiv preprint, abs/1603.07285.\n",
    "* Hochreiter, S. (1991), \"Untersuchungen zu dynamischen neuronalen Netzen\", Diploma thesis, Institut für Informatik, Lehrstuhl Prof. Brauer, Technische Universität München.\n",
    "* Kingma, D. P. & Ba, J. L. (2015), \"Adam: a Method for Stochastic Optimization\", International Conference on Learning Representations, 1–13.\n",
    "* McCulloch, W. & Pitts, W. (1943), \"A Logical Calculus of Ideas Immanent in Nervous Activity\", Bulletin of Mathematical Biophysics, **5**, 127-147. \n",
    "* LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., & Jackel, L. D. (1989) \"Backpropagation Applied to Handwritten Zip Code Recognition\", AT&T Bell Laboratories.\n",
    "* Mitchell, T. (1997), \"Machine Learning\", McGraw-Hill, New York.\n",
    "* Nesterov, Y. (1983), \"A method for unconstrained convex minimization problem with the rate of convergence o(1/k2)\", Doklady ANSSSR (translated as Soviet. Math. Docl.), **269**, 543–547.\n",
    "* Qian, N. (1999), \"On the momentum term in gradient descent learning algorithms\", Neural Networks: The Official Journal of the International Neural Network Society, **12** (1), 145–151.\n",
    "* Robbins, H. and Monro, S. (1951), \"A stochastic approximation method\", *The annals of mathematical statistics*, 400–407.\n",
    "* Rosenblatt, F. (1958), \"The Perceptron: A Probabilistic Model for Information Storage and Organization in The Brain\", Psychological Review, 65-386.\n",
    "* Rosenblatt, F. (1961), \"Principles of Neurodynamics. Perceptrons and the Theory of Brain Mechanisms\", Defense Technical Information Center.\n",
    "* Rumelhart, D. E., McClelland, J. L. and the PDP Research Group (1986a), \"Parallel Distributed Processing: Explorations in the Microstructure of Cognition\", MIT Press, Cambridge.\n",
    "* Rumelhart, D. E., Hinton, G., & Williams, R. (1986b), \"Learning representations by back-propagating errors\", Nature, **323**, 533-536.\n",
    "* Simonyan, K. & Zisserman, A. (2015), \"Very Deep Convolutional Networks for Large-Scale Image Recognition\", in *3rd International Conference on Learning Representations, (ICLR) 2015*, San Diego, CA, USA.\n",
    "* Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014), \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", Journal of Machine Learning Research, **15**, 1929-1958."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
